---
title: "EDA and predicting survival of patients with Heart Failure"
knit: (function(input_file, encoding) {
  out_dir = 'docs';
  rmarkdown::render(input_file, 
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The Data

```{r}
library(ggplot2)
library(cowplot) # for arranging multiple plots
library(dplyr) # for data manipulation
```

Import the data and change some of the column names.

```{r}
df = read.csv("../S1Data.csv")

colnames(df)[1] = "Time"
colnames(df)[2] = "Death_Event" # if the patient deceased during the follow up period. 0 = survived, 1 = died
colnames(df)[6] = "High_Blood_Pressure"
colnames(df)[9] = "Ejection_Fraction"
colnames(df)[12] = "Platelets"
colnames(df)[13] = "CPK_Level"
```

Get a glimpse of the dataset.

```{r}
str(df)
```

The dataset contains a mix of variable types. There are 13 features in total, with 299 observations. Binary features include Gender (0=female, 1=male), Smoking, Diabetes, High Blood Pressure, Anaemia, and the target variable Death Event. Time, Age, Ejection Fraction, Sodium, Creatinine, Platelets, and CPK Level are numeric features. 

Plots will be generated using ggplot. Before creating the plots, check for any missing values in the dataset.

```{r}
colSums(is.na(df))
```

## Stacked barplots of binary features

```{r echo=FALSE}
# make Death_Event a factor variable
df$Death_Event = factor(df$Death_Event)
```

## Anaemia

```{r}
# create temp var to grab the legend for formatting issue
temp=ggplot(df, aes(x=factor(Anaemia), fill=Death_Event)) +
  geom_bar(stat="count", position="stack", width=0.75) +
  geom_label(stat="count", aes(label=..count..), 
             position=position_stack(vjust=0.5)) +
  scale_x_discrete(name="Anaemia", labels=c("0", "1")) +
  guides(fill=guide_legend(override.aes=aes(label=""))) 

legend=get_legend(temp)

# anaemia
anaem=temp + theme(legend.position="none")
```

## Diabetes

```{r}
# diabetes
diab=ggplot(df, aes(x=factor(Diabetes), fill=Death_Event)) +
  geom_bar(stat="count", position="stack", width=0.75) +
  geom_label(stat="count", aes(label=..count..),
             position=position_stack(vjust=0.5)) +
  scale_x_discrete(name="Diabetes", labels=c("0", "1")) +
  theme(legend.position="none")
```

## High Blood Pressure

```{r}
# high blood pressure
hbp=ggplot(df, aes(x=factor(High_Blood_Pressure),   
                   fill=Death_Event)) +
  geom_bar(stat="count", position="stack", width=0.75) +
  geom_label(stat="count", aes(label=..count..),
             position=position_stack(vjust=0.5)) +
  scale_x_discrete(name="High_Blood_Pressure", 
                   labels=c("0", "1")) +
  theme(legend.position="none")

```

## Sex (0=Female, 1=Male)

```{r}
# sex
sex=ggplot(df, aes(x=factor(Gender), fill=Death_Event)) +
  geom_bar(stat="count", position="stack", width=0.75) +
  geom_label(stat="count", aes(label=..count..),
             position=position_stack(vjust=0.5)) +
  scale_x_discrete(name="Sex", labels=c("0", "1")) +
  theme(legend.position="none")
```

## Smoking 

```{r}
# smoking
smoke=ggplot(df, aes(x=factor(Smoking), fill=Death_Event)) +
  geom_bar(stat="count", position="stack", width=0.75) +
  geom_label(stat="count", aes(label=..count..),
             position=position_stack(vjust=0.5)) +
  scale_x_discrete(name="Smoking", labels=c("0", "1")) +
  theme(legend.position="none")
```

## Death Event (0=Survived, 1=Deceased)

```{r}
# death event 
event=ggplot(df, aes(x=factor(Death_Event), fill=Death_Event)) +
  geom_bar(stat="count", width=0.75) +
  geom_label(stat="count", aes(label=..count..),
             position=position_stack(vjust=0.5)) +
  scale_x_discrete(name="Death_Event") +
  theme(legend.position="none")
```

## Counts of binary states across binary features

```{r echo=FALSE}
col1 = plot_grid(anaem, sex, ncol=1)
col2 = plot_grid(diab, smoke, ncol=1)
col3 = plot_grid(hbp, event, ncol=1)
col4 = plot_grid(NULL, legend, NULL, ncol=1)
plot_grid(col1, col2, col3, col4,
          rel_widths=c(1,1,1,0.8),
          nrow=1)  
# rel_widths=c(1,1,1,1.3)
prop.table(table(df$Death_Event))
```

The above plots detail the counts of each class within each binary feature, which are further divided into patients who survived (pink) and patients who died (blue). 

Non-anemics and anemics share close to equal proportions, as is the case for diabetics and non-diabetics. The remaining binary features show unbalance among each of their classes. For class 0 and 1 of each feature, patient survival (Death_Event=0) is the majority outcome. 

The target variable (Death_Event) is not balanced. About 68% (203) of patients survived while around 32% (96) died. 

## Boxplot and density plots of numeric features

## Age

```{r}
# age
age1=ggplot(df, aes(x=Death_Event, y=Age, color=Death_Event)) +
  geom_boxplot() +
  theme(legend.position="top")

age2=ggplot(df, aes(x=Age, fill=Death_Event)) +
  geom_density(alpha=0.5) + 
  theme(legend.position="top")

plot_grid(age1, age2, nrow=1)
```

The density plot for Age seems to indicate a higher chance of death for patients aged 70 and older. 

## Creatinine Phosphokinase

```{r}
# CPK_Level
CPK1=ggplot(df, aes(x=Death_Event, y=CPK_Level, color=Death_Event)) +
  geom_boxplot() +
  theme(legend.position="top")

CPK2=ggplot(df, aes(x=CPK_Level, fill=Death_Event)) +
  geom_density(alpha=0.5) +
  theme(legend.position="top") 

plot_grid(CPK1, CPK2, nrow=1)
```

The box plot above shows many outliers for CPK_Level of patients who survived and patients who died. The density plot indicates positive skew for both classes of Death_Event. 

## Ejection Fraction

```{r}
# Ejection_Fraction
EJ1=ggplot(df, aes(x=Death_Event, y=Ejection_Fraction, color=Death_Event)) +
  geom_boxplot() +
  theme(legend.position="top") 

EJ2=ggplot(df, aes(x=Ejection_Fraction, fill=Death_Event)) +
  geom_density(alpha=0.5) +
  theme(legend.position="top") 

plot_grid(EJ1, EJ2, nrow=1)
```

The density plot of Ejection_Fraction indicates patients that pump less than 30% blood for each contraction may have a higher risk of death. 

## Platelets

```{r}
# Platelets
plat1=ggplot(df, aes(x=Death_Event, y=Platelets, color=Death_Event)) +
  geom_boxplot() +
  theme(legend.position="top") 

plat2=ggplot(df, aes(x=Platelets, fill=Death_Event)) +
  geom_density(alpha=0.5) + 
  theme(legend.position="top") 

plot_grid(plat1, plat2, nrow=1)
```

The box plot of Death_Event and Platelets shows many outliers in the number of Platelets for patients who survived and two outliers in patients who died. The density plot shows that for each Death_Event the number of Platelets loosely follow a normal distribution. 

## Serum Creatinine

```{r}
# Serum Creatinine
cret1=ggplot(df, aes(x=Death_Event, y=Creatinine,
                     color=Death_Event)) +
  geom_boxplot() +
  theme(legend.position="top") 

cret2=ggplot(df, aes(x=Creatinine, fill=Death_Event)) +
  geom_density(alpha=0.5) +
  theme(legend.position="top") 

plot_grid(cret1, cret2, nrow=1)
```

As was the case for some of the other plots, outliers are observed for Serum Creatinine levels in patients who survived/died. The density plot is skewed positively. Deaths seem to overtake survivors with values of Creatinine greater than 1.25.

## Sodium

```{r}
# Sodium
na1=ggplot(df, aes(x=Death_Event, y=Sodium, color=Death_Event))+
  geom_boxplot() +
  theme(legend.position="top") 

na2=ggplot(df, aes(x=Sodium, fill=Death_Event)) +
  geom_density(alpha=0.5) +
  theme(legend.position="top") 

plot_grid(na1, na2, nrow=1)
```

Sodium levels appear to have a similar distribution among patients who survived and patients who died. There is slight negative skew in the plots for patients who survived. 

## Time (days)

```{r}
# Time
t1=ggplot(df, aes(x=Death_Event, y=Time, color=Death_Event)) +
  geom_boxplot() +
  theme(legend.position="top") 

t2=ggplot(df, aes(x=Time, fill=Death_Event)) +
  geom_density(alpha=0.5) +
  theme(legend.position="top") 

plot_grid(t1, t2, nrow=1)
```

From the plots above, patients having a shorter follow up time seem to be associated with death, while patients with a longer follow up time survived. 

## How are the features in the dataset each related to Death Event?

The Mann-Whitney U test will be used to rank each of the features. We will later use machine learning to also rank the features, then compare the results of each method.

```{r}
# p-val < 0.05 indicates the predictor variable strongly relates to death event
columnnames = colnames(df)
pvals = c()
for (i in columnnames) {
  if (i == "Death_Event") {
    next 
  }
  temp = wilcox.test(df[[i]]~Death_Event, df)
  pvals[i] = temp$p.value
}
print(pvals)
```

The variables most strongly associated with Death_Event (having a p-value < 0.05) are Time, Creatinine, Ejection_Fraction, Age, and Sodium. However, including the Time variable when training the models is not practical and doesn't give any insight into predicting the survival of patients. Therefore Time will be omitted from being used as a predictor in this analysis. 

```{r}
# remove time from the df 
df = subset(df, select=-Time)
```

## Splitting the data

In order to train each model the data needs to be split into training and testing data. Using initial_split from the rsample package will allow for the target features proportions to be conserved.

```{r}
library(rsample) # initial_split
set.seed(100)
# split data into 70% training and 30% testing
df_split = initial_split(df, prop = 0.7, strata = Death_Event)
train = training(df_split)
test = testing(df_split)

# check that the proportions are the same in each set
prop.table(table(train$Death_Event))

prop.table(table(test$Death_Event))
```

Using the caret package, models will be trained and tuned. Cross-Validation will be performed on the training data to optimize selection of hyperparameters, with final validation done on the testing data. 

```{r}
library(caret) # training the models
#library(tidyverse)
train$Death_Event = factor(train$Death_Event)
test$Death_Event = factor(test$Death_Event)

set.seed(100)
trctrl = trainControl(method="cv", number=9, verboseIter=FALSE) 
```

## Decision Tree

```{r}
# build the decision tree
set.seed(100)
dt = train(Death_Event~., data=train, method="rpart",
           trControl=trctrl, 
           tuneGrid=expand.grid(cp=c(0.001,0.01, 0.03, 0.3, 1)))
print(dt)
as_tibble(dt$results[which.max(dt$results[,2]),])
```

## Random Forests

```{r}
# build the random forests model
set.seed(100)
rf = train(Death_Event~., data=train, method="ranger",
           trControl=trctrl,
           tuneGrid=expand.grid(mtry=c(2,3,4,5),
                                min.node.size=c(5,10,20,40),
                                splitrule=c("gini",
                                            "extratrees")))
print(rf)
as_tibble(rf$results[which.max(rf$results[,4]),])
```

## Support Vector Machine

```{r}
# build the support vector machine model
set.seed(100)
svm = train(Death_Event~., data=train, method="svmLinear",
            verbose=FALSE, trControl=trctrl, 
            preProcess = c("center", "scale"),
            tuneGrid=expand.grid(C=c(0.001,0.01,0.1,1,5,10))) 
print(svm)
as_tibble(svm$results[which.max(svm$results[,3]),])
```

## Gradient Boosting Machine

```{r}
train_gbm = training(df_split)
test_gbm = testing(df_split)
train_gbm$Death_Event = ifelse(
  train_gbm$Death_Event==0,"Survived", "Deceased")
test_gbm$Death_Event = ifelse(
  test_gbm$Death_Event==0,"Survived", "Deceased")

set.seed(100)
# let the algorithm try out randomized params instead of tuning
# ourselves
gbm = train(Death_Event~., 
            data=train_gbm, 
            method="gbm",
            verbose=FALSE, 
            trControl=trctrl,
            tuneLength=30)
gbm 
# n.trees=50, interaction.depth=1, shrinkage=0.1, n.min...=10
# accuracy: 0.7409603
as_tibble(gbm$results[which.max(gbm$results[,5]),])

```

## Results

```{r}
# dt
pred_dt = predict(dt, test)
cm_dt = confusionMatrix(pred_dt, test$Death_Event, positive="1")
cm_dt
# 81%
cm_dt$byClass 
# Sensitivity: 0.6896552
# Specificity: 0.8688525 
# Recall: 0.6896552 
# Precision: 0.7142857 
```

The Decision Tree was able to achieve an accuracy of 81% and precision 71%.

```{r}
# rf
pred_rf = predict(rf, test)
cm_rf = confusionMatrix(pred_rf, test$Death_Event, positive="1")
cm_rf
# 82% accuracy
cm_rf$byClass
# Sensitivity: 0.5517241
# Specificity: 0.9508197
# Recall: 0.5517241
# Precision: 0.8421053 -> models that are precise and accurate 
# are repeatable and close to true values 
```

Using Random Forests an accuracy of 82% and precision of 84% was accomplished.

```{r}
# svm
pred_svm = predict(svm, test)
cm_svm = confusionMatrix(pred_svm, test$Death_Event,
                         positive="1")
cm_svm
# 72%
cm_svm$byClass
# Sensitivity: 0.3103448
# Specificity: 0.9180328
# Recall: 0.3103448
# Precision: 0.6428571
```

So far the Support Vector Machine has performed the worst, with an accuracy of 72% and precision of 64%.

```{r}
# gbm
pred_gbm = predict(gbm, test_gbm)
cm_gbm = confusionMatrix(pred_gbm, factor(test_gbm$Death_Event))
cm_gbm
cm_gbm$byClass
# Accuracy: 0.8
# Sensitivity: 0.5172414
# Specificity: 0.9344262
# Recall: 0.5172414
# Precision: 0.7894737
```

Gradient Boosting performed well, an accuracy of 80% and precision of 79% was recorded. 

To revisit the feature importance, see how GBM ranked the most important features in the dataset. 

```{r}
gbm_sum = summary(gbm)
gbm_sum
```

The results from the Mann-Whitney U test listed Creatinine, Ejection Fraction, Age, and Sodium as the most important features (disregarding Time) in order of importance. Interesting enough, the GB model listed Creatinine, Ejection Fraction, Age, and Sodium in the same order as the ranking in the Mann-Whitney U test. 



